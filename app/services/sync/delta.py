"""
Sistema de sincronizaÃ§Ã£o incremental (delta) baseado em timestamps.
"""
import asyncio
from datetime import datetime
from typing import Any

from app.arkmeds_client.client import ArkmedsClient
from app.core.db import get_conn
from app.core.logging import app_logger as logger
from app.services.sync_jobs import create_job, finish_job, has_running_job, update_job

from ._upsert import (
    ProgressTracker,
    RateLimiter,
    get_last_sync_info,
    update_sync_state,
    upsert_records,
)


class IncrementalSync:
    """Gerencia sincronizaÃ§Ã£o incremental de dados baseada em timestamps."""

    def __init__(self, client: ArkmedsClient):
        self.client = client
        self.rate_limiter = RateLimiter()

    async def sync_orders_incremental(self, **filters) -> int:
        """
        Sincroniza apenas ordens novas/modificadas desde Ãºltimo sync.
        
        Args:
            **filters: Filtros adicionais
        
        Returns:
            int: NÃºmero de registros sincronizados
        """
        logger.log_info("ðŸ”„ Iniciando sincronizaÃ§Ã£o incremental de ordens...")

        try:
            conn = get_conn()

            # Obter informaÃ§Ãµes do Ãºltimo sync
            last_sync = get_last_sync_info(conn, 'orders')

            # Determinar ponto de partida para delta
            delta_filters = dict(filters)

            if last_sync and last_sync.get('last_updated_at'):
                # Usar timestamp se disponÃ­vel
                delta_filters['updated_at__gt'] = last_sync['last_updated_at']
                logger.log_info(f"ðŸ“… Buscando ordens atualizadas apÃ³s {last_sync['last_updated_at']}")

            elif last_sync and last_sync.get('last_id'):
                # Fallback para ID se nÃ£o hÃ¡ timestamp
                delta_filters['id__gt'] = last_sync['last_id']
                logger.log_info(f"ðŸ”¢ Buscando ordens com ID > {last_sync['last_id']}")

            else:
                # Primeira sincronizaÃ§Ã£o - buscar apenas Ãºltimas 24h para nÃ£o sobrecarregar
                from datetime import timedelta
                yesterday = datetime.now() - timedelta(days=1)
                delta_filters['data_criacao__gte'] = yesterday.date()
                logger.log_info("ðŸ†• Primeira sincronizaÃ§Ã£o - buscando Ãºltimas 24h")

            # Buscar dados incrementais
            new_orders = await self._fetch_incremental_data('chamados', delta_filters)

            if not new_orders:
                logger.log_info("ðŸ“‹ Nenhuma ordem nova para sincronizar")
                return 0

            logger.log_info(f"ðŸ“‹ Encontradas {len(new_orders):,} ordens para sincronizar")

            # Preparar progresso
            progress = ProgressTracker(len(new_orders), "Sincronizando ordens")

            def progress_callback(current, total):
                progress.update(current, total)

            # Converter para formato do banco
            records = []
            for order in new_orders:
                record = order.model_dump() if hasattr(order, 'model_dump') else order
                records.append(record)

            # Fazer upsert
            processed = upsert_records(conn, 'orders', records, progress_callback)

            # Atualizar estado de sync
            if records:
                # Encontrar Ãºltimo registro por timestamp ou ID
                if any(r.get('updated_at') for r in records):
                    last_record = max(
                        (r for r in records if r.get('updated_at')),
                        key=lambda r: r['updated_at']
                    )
                    last_updated = last_record.get('updated_at')
                    last_id = last_record.get('id')
                else:
                    last_record = max(records, key=lambda r: r.get('id', 0))
                    last_updated = None
                    last_id = last_record.get('id')

                # Somar com total anterior se houver
                total_records = processed
                if last_sync and last_sync.get('total_records'):
                    total_records += last_sync['total_records']

                update_sync_state(
                    conn, 'orders',
                    last_updated_at=last_updated,
                    last_id=last_id,
                    total_records=total_records,
                    sync_type='incremental'
                )

            progress.complete()
            return processed

        except Exception as e:
            logger.log_error(f"âŒ Erro durante sync incremental de ordens: {e}")
            raise

    async def sync_equipments_incremental(self, **filters) -> int:
        """
        Sincroniza apenas equipamentos novos/modificados.
        
        Args:
            **filters: Filtros adicionais
        
        Returns:
            int: NÃºmero de registros sincronizados
        """
        logger.log_info("ðŸ”„ Iniciando sincronizaÃ§Ã£o incremental de equipamentos...")

        try:
            conn = get_conn()
            last_sync = get_last_sync_info(conn, 'equipments')

            # Determinar filtros incrementais
            delta_filters = dict(filters)

            if last_sync and last_sync.get('last_updated_at'):
                delta_filters['updated_at__gt'] = last_sync['last_updated_at']
                logger.log_info(f"ðŸ“… Buscando equipamentos atualizados apÃ³s {last_sync['last_updated_at']}")

            elif last_sync and last_sync.get('last_id'):
                delta_filters['id__gt'] = last_sync['last_id']
                logger.log_info(f"ðŸ”¢ Buscando equipamentos com ID > {last_sync['last_id']}")

            else:
                logger.log_info("ðŸ†• Primeira sincronizaÃ§Ã£o de equipamentos")

            # Buscar dados incrementais
            new_equipments = await self._fetch_incremental_data('equipments', delta_filters)

            if not new_equipments:
                logger.log_info("ðŸ”§ Nenhum equipamento novo para sincronizar")
                return 0

            logger.log_info(f"ðŸ”§ Encontrados {len(new_equipments):,} equipamentos para sincronizar")

            # Processar sincronizaÃ§Ã£o
            progress = ProgressTracker(len(new_equipments), "Sincronizando equipamentos")

            def progress_callback(current, total):
                progress.update(current, total)

            records = []
            for equip in new_equipments:
                record = equip.model_dump() if hasattr(equip, 'model_dump') else equip
                records.append(record)

            processed = upsert_records(conn, 'equipments', records, progress_callback)

            # Atualizar sync state
            if records:
                if any(r.get('updated_at') for r in records):
                    last_record = max(
                        (r for r in records if r.get('updated_at')),
                        key=lambda r: r['updated_at']
                    )
                    last_updated = last_record.get('updated_at')
                    last_id = last_record.get('id')
                else:
                    last_record = max(records, key=lambda r: r.get('id', 0))
                    last_updated = None
                    last_id = last_record.get('id')

                total_records = processed
                if last_sync and last_sync.get('total_records'):
                    total_records += last_sync['total_records']

                update_sync_state(
                    conn, 'equipments',
                    last_updated_at=last_updated,
                    last_id=last_id,
                    total_records=total_records,
                    sync_type='incremental'
                )

            progress.complete()
            return processed

        except Exception as e:
            logger.log_error(f"âŒ Erro durante sync incremental de equipamentos: {e}")
            raise

    async def sync_technicians_incremental(self, **filters) -> int:
        """
        Sincroniza apenas tÃ©cnicos novos/modificados.
        
        Args:
            **filters: Filtros adicionais
        
        Returns:
            int: NÃºmero de registros sincronizados
        """
        logger.log_info("ðŸ”„ Iniciando sincronizaÃ§Ã£o incremental de tÃ©cnicos...")

        try:
            conn = get_conn()
            last_sync = get_last_sync_info(conn, 'technicians')

            # Determinar filtros incrementais
            delta_filters = dict(filters)

            if last_sync and last_sync.get('last_updated_at'):
                delta_filters['updated_at__gt'] = last_sync['last_updated_at']
                logger.log_info(f"ðŸ“… Buscando tÃ©cnicos atualizados apÃ³s {last_sync['last_updated_at']}")

            elif last_sync and last_sync.get('last_id'):
                delta_filters['id__gt'] = last_sync['last_id']
                logger.log_info(f"ðŸ”¢ Buscando tÃ©cnicos com ID > {last_sync['last_id']}")

            else:
                logger.log_info("ðŸ†• Primeira sincronizaÃ§Ã£o de tÃ©cnicos")

            # Buscar dados incrementais
            new_technicians = await self._fetch_incremental_data('technicians', delta_filters)

            if not new_technicians:
                logger.log_info("ðŸ‘¥ Nenhum tÃ©cnico novo para sincronizar")
                return 0

            logger.log_info(f"ðŸ‘¥ Encontrados {len(new_technicians):,} tÃ©cnicos para sincronizar")

            # Processar sincronizaÃ§Ã£o
            progress = ProgressTracker(len(new_technicians), "Sincronizando tÃ©cnicos")

            def progress_callback(current, total):
                progress.update(current, total)

            records = []
            for tech in new_technicians:
                record = tech.model_dump() if hasattr(tech, 'model_dump') else tech
                records.append(record)

            processed = upsert_records(conn, 'technicians', records, progress_callback)

            # Atualizar sync state
            if records:
                if any(r.get('updated_at') for r in records):
                    last_record = max(
                        (r for r in records if r.get('updated_at')),
                        key=lambda r: r['updated_at']
                    )
                    last_updated = last_record.get('updated_at')
                    last_id = last_record.get('id')
                else:
                    last_record = max(records, key=lambda r: r.get('id', 0))
                    last_updated = None
                    last_id = last_record.get('id')

                total_records = processed
                if last_sync and last_sync.get('total_records'):
                    total_records += last_sync['total_records']

                update_sync_state(
                    conn, 'technicians',
                    last_updated_at=last_updated,
                    last_id=last_id,
                    total_records=total_records,
                    sync_type='incremental'
                )

            progress.complete()
            return processed

        except Exception as e:
            logger.log_error(f"âŒ Erro durante sync incremental de tÃ©cnicos: {e}")
            raise

    async def sync_all_incremental(self, **filters) -> dict[str, int]:
        """
        Executa sincronizaÃ§Ã£o incremental para todos os recursos.
        
        Args:
            **filters: Filtros adicionais
        
        Returns:
            Dict com contadores por recurso
        """
        logger.log_info("ðŸš€ Iniciando sincronizaÃ§Ã£o incremental completa...")

        results = {}

        try:
            # Sincronizar em sequÃªncia para controlar carga na API
            results['orders'] = await self.sync_orders_incremental(**filters)
            await asyncio.sleep(1)  # Pausa entre recursos

            results['equipments'] = await self.sync_equipments_incremental(**filters)
            await asyncio.sleep(1)

            results['technicians'] = await self.sync_technicians_incremental(**filters)

            total = sum(results.values())
            logger.log_info(f"ðŸŽ‰ SincronizaÃ§Ã£o incremental completa! Total: {total:,} novos registros")

            return results

        except Exception as e:
            logger.log_error(f"âŒ Erro durante sincronizaÃ§Ã£o incremental completa: {e}")
            raise

    async def _fetch_incremental_data(
        self,
        resource_type: str,
        filters: dict[str, Any]
    ) -> list[Any]:
        """
        Busca dados incrementais com base nos filtros.
        
        Args:
            resource_type: Tipo do recurso
            filters: Filtros incluindo condiÃ§Ãµes de delta
        
        Returns:
            Lista com registros novos/modificados
        """
        try:
            # Aplicar rate limiting
            await self.rate_limiter.wait()

            # Mapeamento de tipos para mÃ©todos do client
            method_map = {
                'chamados': self.client.list_chamados,
                'orders': self.client.list_chamados,
                'equipments': getattr(self.client, 'list_equipments', None),
                'technicians': getattr(self.client, 'list_technicians', None)
            }

            fetch_method = method_map.get(resource_type)
            if not fetch_method:
                logger.log_warning(f"âš ï¸ MÃ©todo nÃ£o encontrado para {resource_type}")
                return []

            # Buscar dados
            records = await fetch_method(filters)

            self.rate_limiter.on_success()
            return records if isinstance(records, list) else []

        except Exception as e:
            logger.log_error(f"âŒ Erro buscando dados incrementais de {resource_type}: {e}")
            self.rate_limiter.on_error()
            await self.rate_limiter.wait()
            raise


# FunÃ§Ãµes de conveniÃªncia para uso direto
async def run_incremental_sync(
    client: ArkmedsClient,
    resources: list[str] = None,
    **filters
) -> dict[str, int]:
    """
    Executa sincronizaÃ§Ã£o incremental para recursos especificados.
    
    Args:
        client: Cliente da API
        resources: Lista de recursos ('orders', 'equipments', 'technicians')
        **filters: Filtros adicionais
    
    Returns:
        Dict com resultados por recurso
    """
    if resources is None:
        resources = ['orders', 'equipments', 'technicians']

    sync = IncrementalSync(client)
    results = {}

    for resource in resources:
        try:
            if resource == 'orders':
                results[resource] = await sync.sync_orders_incremental(**filters)
            elif resource == 'equipments':
                results[resource] = await sync.sync_equipments_incremental(**filters)
            elif resource == 'technicians':
                results[resource] = await sync.sync_technicians_incremental(**filters)
            else:
                logger.log_warning(f"âš ï¸ Recurso desconhecido: {resource}")
                results[resource] = 0

            # Pausa entre recursos
            await asyncio.sleep(1)

        except Exception as e:
            logger.log_error(f"âŒ Falha na sincronizaÃ§Ã£o incremental de {resource}: {e}")
            results[resource] = 0

    return results


def should_run_incremental_sync(resource: str, max_age_hours: int = 2) -> bool:
    """
    Verifica se deve executar sincronizaÃ§Ã£o incremental baseado na idade dos dados.
    
    Args:
        resource: Nome do recurso
        max_age_hours: Idade mÃ¡xima em horas antes de sincronizar
    
    Returns:
        bool: True se deve sincronizar
    """
    try:
        conn = get_conn()
        last_sync = get_last_sync_info(conn, resource)

        if not last_sync:
            return True  # Nunca sincronizou

        # Verificar idade do Ãºltimo sync
        from datetime import datetime, timedelta

        # Garantir que synced_at Ã© uma string vÃ¡lida
        synced_at = last_sync.get('synced_at')
        if not synced_at or not isinstance(synced_at, str):
            return True  # Se nÃ£o hÃ¡ timestamp vÃ¡lido, sincronizar

        last_sync_time = datetime.fromisoformat(synced_at)
        max_age = timedelta(hours=max_age_hours)

        return datetime.now() - last_sync_time > max_age

    except Exception as e:
        logger.log_error(e, {"context": "should_run_incremental_sync"})
        return True  # Em caso de erro, sincronizar por seguranÃ§a


async def run_delta_sync_with_progress(
    client: ArkmedsClient,
    resources: list[str] | None = None
) -> int:
    """
    Executa sincronizaÃ§Ã£o incremental com rastreamento de progresso.
    
    Wrapper que cria job de progresso e instrumenta o processo de sync
    para fornecer feedback visual em tempo real.
    
    Args:
        client: Cliente da API
        resources: Lista de recursos para sincronizar (default: ['orders'])
        
    Returns:
        int: NÃºmero total de registros sincronizados
    """
    # Evitar jobs concorrentes
    if has_running_job('delta'):
        logger.log_info("ðŸ”„ Job delta jÃ¡ em execuÃ§Ã£o, pulando...")
        return 0

    job_id = create_job('delta')
    total_synced = 0

    try:
        if resources is None:
            resources = ['orders']

        logger.log_info(f"ðŸš€ Iniciando sincronizaÃ§Ã£o delta com progresso - Job: {job_id}")

        # Estimativa inicial (serÃ¡ refinada durante o processo)
        estimated_total = len(resources) * 100  # Estimativa conservadora
        update_job(job_id, 0, estimated_total)

        sync_manager = IncrementalSync(client)
        processed_items = 0

        for i, resource in enumerate(resources):
            logger.log_info(f"ðŸ“‹ Sincronizando recurso {i+1}/{len(resources)}: {resource}")

            try:
                if resource == 'orders':
                    count = await sync_manager.sync_orders_incremental()
                elif resource == 'equipments':
                    count = await sync_manager.sync_equipments_incremental()
                elif resource == 'technicians':
                    count = await sync_manager.sync_technicians_incremental()
                else:
                    logger.log_info(f"âš ï¸ Recurso desconhecido: {resource}")
                    count = 0

                total_synced += count
                processed_items += count

                # Atualizar progresso (ajustar total se necessÃ¡rio)
                if count > estimated_total // len(resources):
                    # Se recebemos mais dados que esperado, ajustar total
                    estimated_total = processed_items + ((len(resources) - i - 1) * count)

                update_job(job_id, processed_items, estimated_total)

                logger.log_info(f"âœ… {resource}: {count:,} registros sincronizados")

            except Exception as e:
                logger.log_error(e, {"context": f"sync_{resource}", "job_id": job_id})
                # Continuar com outros recursos mesmo se um falhar
                continue

        # Finalizar com sucesso
        finish_job(job_id, 'success')
        logger.log_info(f"ðŸŽ‰ SincronizaÃ§Ã£o delta concluÃ­da: {total_synced:,} registros - Job: {job_id}")

        return total_synced

    except Exception as e:
        finish_job(job_id, 'error')
        logger.log_error(e, {"context": "run_delta_sync_with_progress", "job_id": job_id})
        raise
